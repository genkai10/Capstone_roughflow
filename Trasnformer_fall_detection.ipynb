{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/genkai10/Capstone_roughflow/blob/main/Trasnformer_fall_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision numpy opencv-python\n"
      ],
      "metadata": {
        "id": "OS-cUTKBRfU4",
        "outputId": "d560446c-9144-438e-943f-485d9b5dae02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "3TosGcU-R5fM",
        "outputId": "11a212e6-5cf9-4b7d-fe4e-e00ad631c5fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('/content/drive/MyDrive/smarthome')"
      ],
      "metadata": {
        "id": "YIZ2y5rzSaZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a4eef1d-a4e1-4430-877a-1b20e8d6d76a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.DS_Store', 'eating', 'laydown', 'walking', 'sitting', 'fall', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "qVhRQm6LdPmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_dir, labels, transform=None, sequence_length=16):\n",
        "        self.video_dir = video_dir\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.sequence_length = sequence_length\n",
        "        self.video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_file = self.video_files[idx]\n",
        "        video_path = os.path.join(self.video_dir, video_file)\n",
        "        label = self.labels[video_file]\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        while len(frames) < self.sequence_length:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if self.transform:\n",
        "                frame = self.transform(frame)\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "\n",
        "        while len(frames) < self.sequence_length:\n",
        "            frames.append(frames[-1])\n",
        "\n",
        "        video_tensor = torch.stack(frames)\n",
        "        return video_tensor, label\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "video_dir = '/content/drive/My Drive/smarthome'\n",
        "\n",
        "dataset = VideoDataset(video_dir, labels, transform=transform, sequence_length=16)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "'''"
      ],
      "metadata": {
        "id": "WiSSjzZKYnU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_labels_dict(video_dir, label_mapping):\n",
        "    labels = {}\n",
        "    for label_name, label_index in label_mapping.items():\n",
        "        label_path = os.path.join(video_dir, label_name)\n",
        "        if os.path.exists(label_path):\n",
        "            for video_file in os.listdir(label_path):\n",
        "                if video_file.endswith('.mp4'):\n",
        "                    labels[os.path.join(label_name, video_file)] = label_index\n",
        "    return labels\n",
        "\n",
        "label_mapping = {\n",
        "    'sitting': 0,\n",
        "    'eating': 1,\n",
        "    'fall': 2,\n",
        "    'laydown': 3,\n",
        "    'walking': 4,\n",
        "}\n",
        "\n",
        "# Define the path to your dataset directory\n",
        "video_dir = '/content/drive/MyDrive/smarthome'\n",
        "labels = generate_labels_dict(video_dir, label_mapping)\n",
        "\n",
        "# Print the labels dictionary to verify\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK7Wwxk2cIxW",
        "outputId": "5bafba91-2fc0-4c5d-d56f-b4796bc3d2ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sitting/Sitdown_p06_r07_v06_c05.mp4': 0, 'sitting/Sitdown_p07_r01_v06_c05.mp4': 0, 'sitting/Sitdown_p06_r03_v06_c04.mp4': 0, 'sitting/Sitdown_p06_r02_v27_c01.mp4': 0, 'sitting/Sitdown_p06_r02_v14_c02.mp4': 0, 'sitting/Sitdown_p07_r01_v05_c05.mp4': 0, 'sitting/Sitdown_p07_r00_v04_c05.mp4': 0, 'sitting/Sitdown_p07_r00_v03_c05.mp4': 0, 'sitting/Sitdown_p07_r03_v06_c05.mp4': 0, 'sitting/Sitdown_p07_r01_v09_c02.mp4': 0, 'sitting/Sitdown_p07_r03_v09_c02.mp4': 0, 'sitting/Sitdown_p06_r02_v15_c02.mp4': 0, 'sitting/Sitdown_p06_r02_v10_c02.mp4': 0, 'sitting/Sitdown_p07_r01_v10_c01.mp4': 0, 'sitting/Sitdown_p06_r03_v05_c05.mp4': 0, 'sitting/Sitdown_p07_r01_v04_c05.mp4': 0, 'sitting/Sitdown_p07_r03_v09_c01.mp4': 0, 'sitting/Sitdown_p06_r03_v03_c05.mp4': 0, 'sitting/Sitdown_p06_r02_v11_c01.mp4': 0, 'sitting/Sitdown_p06_r06_v06_c04.mp4': 0, 'sitting/Sitdown_p06_r07_v06_c04.mp4': 0, 'sitting/Sitdown_p07_r01_v09_c01.mp4': 0, 'sitting/Sitdown_p07_r03_v05_c05.mp4': 0, 'sitting/Sitdown_p06_r03_v03_c04.mp4': 0, 'sitting/Sitdown_p07_r00_v01_c05.mp4': 0, 'eating/Eat.Snack_p04_r00_v17_c06.mp4': 1, 'eating/Eat.Snack_p06_r05_v06_c05.mp4': 1, 'eating/Eat.Snack_p03_r03_v16_c06.mp4': 1, 'eating/Eat.Snack_p03_r04_v15_c06.mp4': 1, 'eating/Eat.Snack_p06_r05_v11_c02.mp4': 1, 'eating/Eat.Snack_p06_r06_v06_c05.mp4': 1, 'eating/Eat.Snack_p06_r05_v11_c01.mp4': 1, 'eating/Eat.Snack_p06_r00_v06_c05.mp4': 1, 'eating/Eat.Snack_p04_r00_v17_c07.mp4': 1, 'eating/Eat.Snack_p06_r05_v06_c04.mp4': 1, 'eating/Eat.Snack_p06_r06_v06_c04.mp4': 1, 'eating/Eat.Attable_p19_r19_v07_c01.mp4': 1, 'eating/Eat.Attable_p19_r17_v07_c01.mp4': 1, 'eating/Eat.Attable_p19_r18_v07_c01.mp4': 1, 'eating/Eat.Attable_p19_r20_v07_c01.mp4': 1, 'eating/Eat.Attable_p18_r18_v09_c02.mp4': 1, 'eating/Eat.Attable_p20_r00_v08_c01.mp4': 1, 'eating/Eat.Attable_p18_r21_v09_c02.mp4': 1, 'eating/Eat.Attable_p20_r02_v08_c01.mp4': 1, 'eating/Eat.Attable_p18_r22_v09_c02.mp4': 1, 'eating/Eat.Attable_p20_r03_v08_c01.mp4': 1, 'eating/Eat.Attable_p18_r16_v09_c02.mp4': 1, 'eating/Eat.Attable_p20_r01_v08_c01.mp4': 1, 'eating/Eat.Attable_p18_r15_v09_c02.mp4': 1, 'eating/Eat.Attable_p18_r18_v09_c01.mp4': 1, 'fall/Copy of fall-06-cam0.mp4': 2, 'fall/Copy of fall-04-cam0.mp4': 2, 'fall/Copy of fall-02-cam0.mp4': 2, 'fall/Copy of fall-08-cam0.mp4': 2, 'fall/Copy of fall-05-cam0.mp4': 2, 'fall/Copy of fall-09-cam0.mp4': 2, 'fall/Copy of fall-03-cam0.mp4': 2, 'fall/Copy of fall-01-cam0.mp4': 2, 'fall/Copy of fall-10-cam0.mp4': 2, 'fall/Copy of fall-12-cam0.mp4': 2, 'fall/Copy of fall-19-cam0.mp4': 2, 'fall/Copy of fall-14-cam0.mp4': 2, 'fall/Copy of fall-18-cam0.mp4': 2, 'fall/Copy of fall-16-cam0.mp4': 2, 'fall/Copy of fall-13-cam0.mp4': 2, 'fall/Copy of fall-15-cam0.mp4': 2, 'fall/Copy of fall-20-cam0.mp4': 2, 'fall/Copy of fall-17-cam0.mp4': 2, 'fall/Copy of fall-21-cam0.mp4': 2, 'fall/Copy of fall-22-cam0.mp4': 2, 'fall/Copy of fall-26-cam0.mp4': 2, 'fall/Copy of fall-24-cam0.mp4': 2, 'fall/Copy of fall-30-cam0.mp4': 2, 'fall/Copy of fall-25-cam0.mp4': 2, 'fall/Copy of fall-27-cam0.mp4': 2, 'laydown/Laydown_p17_r01_v05_c04.mp4': 3, 'laydown/Laydown_p15_r00_v06_c04.mp4': 3, 'laydown/Laydown_p15_r00_v07_c04.mp4': 3, 'laydown/Laydown_p15_r02_v02_c05.mp4': 3, 'laydown/Laydown_p15_r00_v02_c04.mp4': 3, 'laydown/Laydown_p17_r03_v05_c05.mp4': 3, 'laydown/Laydown_p15_r02_v02_c04.mp4': 3, 'laydown/Laydown_p15_r01_v07_c05.mp4': 3, 'laydown/Laydown_p17_r02_v05_c05.mp4': 3, 'laydown/Laydown_p15_r00_v06_c05.mp4': 3, 'laydown/Laydown_p15_r00_v05_c05.mp4': 3, 'laydown/Laydown_p17_r00_v05_c04.mp4': 3, 'laydown/Laydown_p15_r00_v05_c04.mp4': 3, 'laydown/Laydown_p15_r00_v02_c05.mp4': 3, 'laydown/Laydown_p14_r05_v02_c04.mp4': 3, 'laydown/Laydown_p14_r07_v02_c04.mp4': 3, 'laydown/Laydown_p16_r01_v05_c05.mp4': 3, 'laydown/Laydown_p16_r00_v05_c04.mp4': 3, 'laydown/Laydown_p14_r06_v02_c04.mp4': 3, 'laydown/Laydown_p14_r04_v02_c05.mp4': 3, 'laydown/Laydown_p16_r00_v05_c05.mp4': 3, 'laydown/Laydown_p14_r07_v02_c05.mp4': 3, 'laydown/Laydown_p14_r06_v02_c05.mp4': 3, 'laydown/Laydown_p14_r03_v02_c05.mp4': 3, 'laydown/Laydown_p14_r04_v02_c04.mp4': 3, 'walking/Walk_p03_r18_v03_c04.mp4': 4, 'walking/Walk_p03_r17_v04_c05.mp4': 4, 'walking/Walk_p03_r18_v04_c04.mp4': 4, 'walking/Walk_p03_r24_v04_c04.mp4': 4, 'walking/Walk_p03_r37_v02_c05.mp4': 4, 'walking/Walk_p03_r33_v04_c04.mp4': 4, 'walking/Walk_p03_r37_v04_c05.mp4': 4, 'walking/Walk_p03_r24_v04_c05.mp4': 4, 'walking/Walk_p03_r24_v03_c05.mp4': 4, 'walking/Walk_p03_r33_v02_c05.mp4': 4, 'walking/Walk_p03_r18_v02_c04.mp4': 4, 'walking/Walk_p03_r25_v02_c04.mp4': 4, 'walking/Walk_p03_r39_v02_c04.mp4': 4, 'walking/Walk_p03_r38_v02_c05.mp4': 4, 'walking/Walk_p03_r24_v02_c05.mp4': 4, 'walking/Walk_p03_r34_v04_c04.mp4': 4, 'walking/Walk_p03_r34_v02_c05.mp4': 4, 'walking/Walk_p03_r24_v03_c04.mp4': 4, 'walking/Walk_p03_r38_v02_c04.mp4': 4, 'walking/Walk_p03_r34_v02_c04.mp4': 4, 'walking/Walk_p03_r39_v02_c05.mp4': 4, 'walking/Walk_p03_r34_v04_c05.mp4': 4, 'walking/Walk_p03_r25_v02_c05.mp4': 4, 'walking/Walk_p03_r33_v04_c05.mp4': 4, 'walking/Walk_p03_r37_v02_c04.mp4': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_video_dir= '/content/drive/MyDrive/smarthome/test'\n",
        "test_labels = generate_labels_dict(test_video_dir, label_mapping)\n",
        "print(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvEQUzLnyJNp",
        "outputId": "82ef1374-df39-4689-db20-d07df03f0461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sitting/Sitdown_p06_r02_v14_c01.mp4': 0, 'sitting/Sitdown_p06_r02_v10_c01.mp4': 0, 'sitting/Sitdown_p06_r02_v11_c02.mp4': 0, 'sitting/Sitdown_p07_r01_v07_c04.mp4': 0, 'sitting/Sitdown_p06_r03_v05_c04.mp4': 0, 'eating/Eat.Snack_p04_r00_v16_c06.mp4': 1, 'eating/Eat.Snack_p06_r00_v06_c04.mp4': 1, 'eating/Eat.Attable_p18_r17_v09_c02.mp4': 1, 'eating/Eat.Attable_p18_r14_v09_c02.mp4': 1, 'eating/Eat.Attable_p18_r16_v09_c01.mp4': 1, 'fall/Copy of fall-07-cam0.mp4': 2, 'fall/Copy of fall-11-cam0.mp4': 2, 'fall/Copy of fall-29-cam0.mp4': 2, 'fall/Copy of fall-28-cam0.mp4': 2, 'fall/Copy of fall-23-cam0.mp4': 2, 'laydown/Laydown_p17_r01_v05_c05.mp4': 3, 'laydown/Laydown_p17_r02_v05_c04.mp4': 3, 'laydown/Laydown_p17_r04_v05_c04.mp4': 3, 'laydown/Laydown_p14_r05_v02_c05.mp4': 3, 'laydown/Laydown_p16_r01_v05_c04.mp4': 3, 'walking/Walk_p03_r17_v04_c04.mp4': 4, 'walking/Walk_p03_r39_v04_c05.mp4': 4, 'walking/Walk_p03_r40_v02_c04.mp4': 4, 'walking/Walk_p03_r18_v02_c05.mp4': 4, 'walking/Walk_p03_r18_v04_c05.mp4': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_dir, labels, transform=None, sequence_length=16):\n",
        "        self.video_dir = video_dir\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.sequence_length = sequence_length\n",
        "        self.video_files = list(labels.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_file = self.video_files[idx]\n",
        "        video_path = os.path.join(self.video_dir, video_file)\n",
        "        label = self.labels[video_file]\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        while len(frames) < self.sequence_length:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if self.transform:\n",
        "                frame = self.transform(frame)\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "\n",
        "        while len(frames) < self.sequence_length:\n",
        "            frames.append(frames[-1])\n",
        "\n",
        "        video_tensor = torch.stack(frames)\n",
        "        return video_tensor, label\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = VideoDataset(video_dir, labels, transform=transform, sequence_length=16)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
      ],
      "metadata": {
        "id": "BSn_UiFcdHl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "class CNNFeatureExtractor(nn.Module):\n",
        "    def __init__(self, output_size):\n",
        "        super(CNNFeatureExtractor, self).__init__()\n",
        "        resnet = resnet50(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(resnet.fc.in_features, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, feature_dim, num_classes, num_layers, num_heads, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.cnn = CNNFeatureExtractor(feature_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, 16, feature_dim))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=feature_dim, nhead=num_heads, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(feature_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, c, h, w = x.size()\n",
        "        x = x.view(batch_size * seq_len, c, h, w)\n",
        "        x = self.cnn(x)\n",
        "        x = x.view(batch_size, seq_len, -1)\n",
        "        x += self.positional_encoding[:, :seq_len, :]\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=0)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerModel(512, 12, 4, 8, 0.1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "4OkLMkC5YriB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35144ec9-43ee-4d43-f5d4-267539e45776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, criterion, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for videos, labels in dataloader:\n",
        "            videos = videos.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(videos)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * videos.size(0)\n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train(model, dataloader, criterion, optimizer, 10)\n"
      ],
      "metadata": {
        "id": "TCdE_xEQYyP3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c4f64f-dc8d-4534-c574-ce2d74f1baad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.2269\n",
            "Epoch 2/10, Loss: 0.2629\n",
            "Epoch 3/10, Loss: 0.3137\n",
            "Epoch 4/10, Loss: 0.2044\n",
            "Epoch 5/10, Loss: 0.6467\n",
            "Epoch 6/10, Loss: 0.6155\n",
            "Epoch 7/10, Loss: 0.5768\n",
            "Epoch 8/10, Loss: 0.3597\n",
            "Epoch 9/10, Loss: 0.2380\n",
            "Epoch 10/10, Loss: 0.2593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for videos, labels in dataloader:\n",
        "            videos = videos.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(videos)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    return accuracy\n",
        "\n",
        "# Assuming you have a test dataloader\n",
        "test_dataset = VideoDataset(test_video_dir, test_labels, transform=transform, sequence_length=16)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "evaluate(model, test_dataloader)\n"
      ],
      "metadata": {
        "id": "iQi2EJFHY0h-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd83cfe4-ad8b-43ef-c60f-e48bdd021a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.92"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "'''\n",
        "test_dataset = VideoDataset(test_video_dir, test_labels, transform=transform, sequence_length=16)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "\n",
        "for i, (videos, labels) in enumerate(test_dataloader):\n",
        "    print(f\"Batch {i+1}\")\n",
        "    print(f\"Videos shape: {videos.shape}\")\n",
        "    print(f\"Labels: {labels}\")\n",
        "    if i == 1:\n",
        "        break\n",
        "'''"
      ],
      "metadata": {
        "id": "v2quxm31SxXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/drive/MyDrive/smarthome/transformer_model_complete.pth'\n",
        "torch.save(model, model_save_path)\n"
      ],
      "metadata": {
        "id": "IzUcpXv4PvR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('/content/drive/MyDrive/smarthome/transformer_model_complete.pth')\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "e-u1YTdVQDEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, (224, 224))\n",
        "        frame = frame / 255.0\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "\n",
        "    video_tensor = torch.tensor(frames, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "    return video_tensor.unsqueeze(0)\n",
        "\n",
        "def predict_label(input_video):\n",
        "    with torch.no_grad():\n",
        "        output = model(input_video)\n",
        "        _, predicted_class = torch.max(output, 1)\n",
        "        return predicted_class.item()\n",
        "\n",
        "\n",
        "input_video_path = ''\n",
        "input_video = preprocess_video(input_video_path)\n",
        "predicted_label = predict_label(input_video)\n",
        "print(\"Predicted Label:\", predicted_label)\n"
      ],
      "metadata": {
        "id": "PUI_VpV0TNR8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}